{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: Pawar Sameer Ashok\n",
    "# Roll no: 19GG20024\n",
    "# Project Title: Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:28:19.484157Z",
     "iopub.status.busy": "2022-10-25T08:28:19.483244Z",
     "iopub.status.idle": "2022-10-25T08:28:25.044092Z",
     "shell.execute_reply": "2022-10-25T08:28:25.043382Z",
     "shell.execute_reply.started": "2022-10-25T08:28:19.484018Z"
    }
   },
   "outputs": [],
   "source": [
    "import os   # handling the files\n",
    "import pickle # storing numpy features\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # how much data is process till now\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input # extract features from image data.\n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**os** - used to handle files using system commands.\n",
    "\n",
    "**pickle** - used to store numpy features extracted\n",
    "\n",
    "**numpy** - used to perform a wide variety of mathematical operations on arrays\n",
    "\n",
    "**tqdm** - progress bar decorator for iterators. Includes a default range iterator printing to stderr.\n",
    "\n",
    "**VGG16, preprocess_input** - imported modules for feature extraction from the image data\n",
    "\n",
    "**load_img, img_to_array** - used for loading the image and converting the image to a numpy array\n",
    "\n",
    "**Tokenizer** - used for loading the text as convert them into a token\n",
    "\n",
    "**pad_sequences** - used for equal distribution of words in sentences filling the remaining spaces with zeros\n",
    "\n",
    "**plot_model** - used to visualize the architecture of the model through different images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we must set the directories to use the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:28:25.047643Z",
     "iopub.status.busy": "2022-10-25T08:28:25.047438Z",
     "iopub.status.idle": "2022-10-25T08:28:25.055958Z",
     "shell.execute_reply": "2022-10-25T08:28:25.055098Z",
     "shell.execute_reply.started": "2022-10-25T08:28:25.047618Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/kaggle/input/flickr8k'\n",
    "WORKING_DIR = '/kaggle/working'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Image Features\n",
    "\n",
    "We have to load and restructure the model\n",
    "\n",
    "VGG-16 is a convolutional neural network that is 16 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database [1]. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:28:25.057787Z",
     "iopub.status.busy": "2022-10-25T08:28:25.05736Z",
     "iopub.status.idle": "2022-10-25T08:28:31.504659Z",
     "shell.execute_reply": "2022-10-25T08:28:31.503852Z",
     "shell.execute_reply.started": "2022-10-25T08:28:25.057745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load vgg16 Model\n",
    "model = VGG16()\n",
    "\n",
    "# restructure model\n",
    "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
    "\n",
    "# Summerize\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Fully connected layer of the VGG16 model is not needed, just the previous layers to extract feature results.\n",
    "\n",
    "+ By preference you may include more layers, but for quicker results avoid adding the unnecessary layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract the image features\n",
    "Now we extract the image features and load the data for preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:28:31.506838Z",
     "iopub.status.busy": "2022-10-25T08:28:31.506513Z",
     "iopub.status.idle": "2022-10-25T08:36:45.745715Z",
     "shell.execute_reply": "2022-10-25T08:36:45.745023Z",
     "shell.execute_reply.started": "2022-10-25T08:28:31.506798Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract features from image\n",
    "features = {}\n",
    "directory = os.path.join(BASE_DIR, 'Images')\n",
    "\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    # load the image from file\n",
    "    img_path = directory + '/' + img_name\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    # convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # preprocess image for vgg\n",
    "    image = preprocess_input(image)\n",
    "    # extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    # get image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "    # store feature\n",
    "    features[image_id] = feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary 'features' is created and will be loaded with the extracted features of image data\n",
    "\n",
    "**load_img(img_path, target_size=(224, 224))** - custom dimension to resize the image when loaded to the array\n",
    "\n",
    "**image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))** - reshaping the image data to preprocess in a RGB type image.\n",
    "\n",
    "**model.predict(image, verbose=0)** - extraction of features from the image\n",
    "\n",
    "**img_name.split('.')[0]** - split of the image name from the extension to load only the image name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:45.748716Z",
     "iopub.status.busy": "2022-10-25T08:36:45.746917Z",
     "iopub.status.idle": "2022-10-25T08:36:46.085215Z",
     "shell.execute_reply": "2022-10-25T08:36:46.08444Z",
     "shell.execute_reply.started": "2022-10-25T08:36:45.748677Z"
    }
   },
   "outputs": [],
   "source": [
    "# store features in pickle\n",
    "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted features are not stored in the disk, so re-extraction of features can extend running time\n",
    "\n",
    "Dumps and store your dictionary in a pickle for reloading it to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.086819Z",
     "iopub.status.busy": "2022-10-25T08:36:46.08648Z",
     "iopub.status.idle": "2022-10-25T08:36:46.249147Z",
     "shell.execute_reply": "2022-10-25T08:36:46.248386Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.086773Z"
    }
   },
   "outputs": [],
   "source": [
    "# load features from pickle\n",
    "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all your stored feature data to your project for quicker runtime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Captions Data\n",
    "\n",
    "Let us store the captions data from the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.252053Z",
     "iopub.status.busy": "2022-10-25T08:36:46.251763Z",
     "iopub.status.idle": "2022-10-25T08:36:46.301755Z",
     "shell.execute_reply": "2022-10-25T08:36:46.301006Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.252013Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we split and append the captions data with the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.30334Z",
     "iopub.status.busy": "2022-10-25T08:36:46.302984Z",
     "iopub.status.idle": "2022-10-25T08:36:46.425087Z",
     "shell.execute_reply": "2022-10-25T08:36:46.424233Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.303303Z"
    }
   },
   "outputs": [],
   "source": [
    "# create mapping of image to captions\n",
    "mapping = {}\n",
    "# process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the line by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    # remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # convert caption list to string\n",
    "    caption = \" \".join(caption)\n",
    "    # create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    # store the caption\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dictionary 'mapping' is created with key as image_id and values as the corresponding caption text\n",
    "\n",
    "+ Same image may have multiple captions, **if image_id not in mapping: mapping[image_id] = []** creates a list for appending captions to the corresponding image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let us see the no. of images loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.427289Z",
     "iopub.status.busy": "2022-10-25T08:36:46.426404Z",
     "iopub.status.idle": "2022-10-25T08:36:46.43505Z",
     "shell.execute_reply": "2022-10-25T08:36:46.43438Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.42725Z"
    }
   },
   "outputs": [],
   "source": [
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.43898Z",
     "iopub.status.busy": "2022-10-25T08:36:46.438245Z",
     "iopub.status.idle": "2022-10-25T08:36:46.44655Z",
     "shell.execute_reply": "2022-10-25T08:36:46.445702Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.438943Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # delete digits, special chars, etc., \n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            # delete additional spaces\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            # add start and end tags to the caption\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined to clean and convert the text for quicker process and better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the text **before** and **after** cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.448294Z",
     "iopub.status.busy": "2022-10-25T08:36:46.44765Z",
     "iopub.status.idle": "2022-10-25T08:36:46.457901Z",
     "shell.execute_reply": "2022-10-25T08:36:46.457088Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.448251Z"
    }
   },
   "outputs": [],
   "source": [
    "# before preprocess of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.460073Z",
     "iopub.status.busy": "2022-10-25T08:36:46.459857Z",
     "iopub.status.idle": "2022-10-25T08:36:46.593062Z",
     "shell.execute_reply": "2022-10-25T08:36:46.592436Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.460043Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.594438Z",
     "iopub.status.busy": "2022-10-25T08:36:46.594196Z",
     "iopub.status.idle": "2022-10-25T08:36:46.599199Z",
     "shell.execute_reply": "2022-10-25T08:36:46.598558Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.594401Z"
    }
   },
   "outputs": [],
   "source": [
    "# after preprocess of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we will store the preprocessed captions into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.600702Z",
     "iopub.status.busy": "2022-10-25T08:36:46.600321Z",
     "iopub.status.idle": "2022-10-25T08:36:46.617779Z",
     "shell.execute_reply": "2022-10-25T08:36:46.61713Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.600668Z"
    }
   },
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.620295Z",
     "iopub.status.busy": "2022-10-25T08:36:46.619921Z",
     "iopub.status.idle": "2022-10-25T08:36:46.631238Z",
     "shell.execute_reply": "2022-10-25T08:36:46.630438Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.620262Z"
    }
   },
   "outputs": [],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. of unique captions stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Captions\n",
    "Let us see the first ten captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.634551Z",
     "iopub.status.busy": "2022-10-25T08:36:46.634305Z",
     "iopub.status.idle": "2022-10-25T08:36:46.641973Z",
     "shell.execute_reply": "2022-10-25T08:36:46.641224Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.634509Z"
    }
   },
   "outputs": [],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing of Text Data\n",
    "Now we start processing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:46.643504Z",
     "iopub.status.busy": "2022-10-25T08:36:46.64326Z",
     "iopub.status.idle": "2022-10-25T08:36:47.238979Z",
     "shell.execute_reply": "2022-10-25T08:36:47.238084Z",
     "shell.execute_reply.started": "2022-10-25T08:36:46.643474Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:47.240685Z",
     "iopub.status.busy": "2022-10-25T08:36:47.240394Z",
     "iopub.status.idle": "2022-10-25T08:36:47.246678Z",
     "shell.execute_reply": "2022-10-25T08:36:47.24559Z",
     "shell.execute_reply.started": "2022-10-25T08:36:47.240625Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:47.248698Z",
     "iopub.status.busy": "2022-10-25T08:36:47.248432Z",
     "iopub.status.idle": "2022-10-25T08:36:47.286034Z",
     "shell.execute_reply": "2022-10-25T08:36:47.285384Z",
     "shell.execute_reply.started": "2022-10-25T08:36:47.248664Z"
    }
   },
   "outputs": [],
   "source": [
    "# get maximum length of the caption available\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Finding the maximum length of the captions, used for reference for the padding sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After preprocessing the data now we will train, test and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:47.287517Z",
     "iopub.status.busy": "2022-10-25T08:36:47.287282Z",
     "iopub.status.idle": "2022-10-25T08:36:47.293013Z",
     "shell.execute_reply": "2022-10-25T08:36:47.292049Z",
     "shell.execute_reply.started": "2022-10-25T08:36:47.287485Z"
    }
   },
   "outputs": [],
   "source": [
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will define a batch and include the padding sequence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:47.294869Z",
     "iopub.status.busy": "2022-10-25T08:36:47.29455Z",
     "iopub.status.idle": "2022-10-25T08:36:47.304955Z",
     "shell.execute_reply": "2022-10-25T08:36:47.303947Z",
     "shell.execute_reply.started": "2022-10-25T08:36:47.294834Z"
    }
   },
   "outputs": [],
   "source": [
    "# create data generator to get data in batch (avoids session crash)\n",
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "                    # store the sequences\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                yield [X1, X2], y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding sequence normalizes the size of all captions to the max size filling them with zeros for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:47.30663Z",
     "iopub.status.busy": "2022-10-25T08:36:47.30636Z",
     "iopub.status.idle": "2022-10-25T08:36:49.354727Z",
     "shell.execute_reply": "2022-10-25T08:36:49.353898Z",
     "shell.execute_reply.started": "2022-10-25T08:36:47.306597Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoder model\n",
    "# image feature layers\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "# sequence feature layers\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# plot the model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **shape=(4096,)** - output length of the features from the VGG model\n",
    "\n",
    "+ **Dense** - single dimension linear layer array\n",
    "\n",
    "+ **Dropout()** - used to add regularization to the data, avoiding over fitting & dropping out a fraction of the data from the layers\n",
    "\n",
    "+ **model.compile()** - compilation of the model\n",
    "\n",
    "+ **loss=’sparse_categorical_crossentropy’** - loss function for category outputs\n",
    "\n",
    "+ **optimizer=’adam’** - automatically adjust the learning rate for the model over the no. of epochs\n",
    "\n",
    "+ Model plot shows the concatenation of the inputs and outputs into a single layer\n",
    "\n",
    "+ Feature extraction of image was already done using VGG, no CNN model was needed in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "Now let us train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:36:49.357113Z",
     "iopub.status.busy": "2022-10-25T08:36:49.35648Z",
     "iopub.status.idle": "2022-10-25T08:59:49.163172Z",
     "shell.execute_reply": "2022-10-25T08:59:49.162465Z",
     "shell.execute_reply.started": "2022-10-25T08:36:49.357075Z"
    }
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "steps = len(train) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create data generator\n",
    "    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    # fit for one epoch\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **steps = len(train) // batch_size** - back propagation and fetch the next data\n",
    "\n",
    "+ Loss decreases gradually over the iterations\n",
    "\n",
    "+ Increase the no. of epochs for better results\n",
    "\n",
    "+ Assign the no. of epochs and batch size accordingly for quicker results\n",
    "\n",
    "\n",
    "### You can save the model in the working directory for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:59:49.166744Z",
     "iopub.status.busy": "2022-10-25T08:59:49.166371Z",
     "iopub.status.idle": "2022-10-25T08:59:49.298439Z",
     "shell.execute_reply": "2022-10-25T08:59:49.297669Z",
     "shell.execute_reply.started": "2022-10-25T08:59:49.166715Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(WORKING_DIR+'/best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Captions for the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:59:49.300279Z",
     "iopub.status.busy": "2022-10-25T08:59:49.299818Z",
     "iopub.status.idle": "2022-10-25T08:59:49.305951Z",
     "shell.execute_reply": "2022-10-25T08:59:49.305226Z",
     "shell.execute_reply.started": "2022-10-25T08:59:49.300238Z"
    }
   },
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convert the predicted index from the model into a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:59:49.307653Z",
     "iopub.status.busy": "2022-10-25T08:59:49.307396Z",
     "iopub.status.idle": "2022-10-25T08:59:49.317604Z",
     "shell.execute_reply": "2022-10-25T08:59:49.316801Z",
     "shell.execute_reply.started": "2022-10-25T08:59:49.307612Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Captiongenerator appending all the words for an image\n",
    "\n",
    "+ The caption starts with 'startseq' and the model continues to predict the caption until the 'endseq' appeared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "Now we validate the data using BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T08:59:49.319138Z",
     "iopub.status.busy": "2022-10-25T08:59:49.318852Z",
     "iopub.status.idle": "2022-10-25T09:05:46.414803Z",
     "shell.execute_reply": "2022-10-25T09:05:46.413909Z",
     "shell.execute_reply.started": "2022-10-25T08:59:49.319106Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions = mapping[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ BLEU Score is used to evaluate the predicted text against a reference text, in a list of tokens.\n",
    "\n",
    "+ The reference text contains all the words appended from the captions data (actual_captions)\n",
    "\n",
    "+ A BLEU Score more than **0.4 is considered a good result**, for a better score increase the no. of epochs accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T09:05:46.418681Z",
     "iopub.status.busy": "2022-10-25T09:05:46.418475Z",
     "iopub.status.idle": "2022-10-25T09:05:46.425676Z",
     "shell.execute_reply": "2022-10-25T09:05:46.424857Z",
     "shell.execute_reply.started": "2022-10-25T09:05:46.418656Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "def generate_caption(image_name):\n",
    "    # load the image\n",
    "    # image_name = \"1001773457_577c3a7d70.jpg\"\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Image caption generator defined\n",
    "\n",
    "+ First prints the actual captions of the image then prints a predicted caption of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T09:05:46.427476Z",
     "iopub.status.busy": "2022-10-25T09:05:46.426759Z",
     "iopub.status.idle": "2022-10-25T09:05:47.14768Z",
     "shell.execute_reply": "2022-10-25T09:05:47.143756Z",
     "shell.execute_reply.started": "2022-10-25T09:05:46.427439Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_caption(\"1001773457_577c3a7d70.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T09:05:47.149803Z",
     "iopub.status.busy": "2022-10-25T09:05:47.14905Z",
     "iopub.status.idle": "2022-10-25T09:05:47.951665Z",
     "shell.execute_reply": "2022-10-25T09:05:47.951003Z",
     "shell.execute_reply.started": "2022-10-25T09:05:47.14977Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_caption(\"1002674143_1b742ab4b8.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T09:05:47.953513Z",
     "iopub.status.busy": "2022-10-25T09:05:47.952994Z",
     "iopub.status.idle": "2022-10-25T09:05:48.403588Z",
     "shell.execute_reply": "2022-10-25T09:05:48.403012Z",
     "shell.execute_reply.started": "2022-10-25T09:05:47.953475Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_caption(\"101669240_b2d3e7f17b.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "\n",
    "+ Training the model by increasing the no. of epochs can give better and more accurate results.\n",
    "\n",
    "+ Processing large amount of data can take a lot of time and system resource.\n",
    "\n",
    "+ The no. of layers of the model can be increased if you want to process large dataset like flickr32k.\n",
    "\n",
    "\n",
    "\n",
    "**In this project , we have built an Image Caption Generator exploring the Flickr Dataset as an advanced deep learning project using different models from image extraction and text based processing.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
